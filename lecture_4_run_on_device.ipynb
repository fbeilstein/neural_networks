{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "runtime_attributes": {
        "runtime_version": "2025.07"
      },
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Very long ~20 mins\n",
        "# Runtime restarts after install\n",
        "\n",
        "import os\n",
        "MARKER_FILE = \"./setup_done\"\n",
        "\n",
        "if not os.path.exists(MARKER_FILE):\n",
        "  !pip install -q poetry\n",
        "  !poetry config virtualenvs.create false\n",
        "  !poetry env use 3.11.13\n",
        "  !poetry init --no-interaction\n",
        "  !poetry add torch torchmetrics torchinfo torchview torchvision\\\n",
        "  onnx==1.14.0 onnxruntime onnx-tf\\\n",
        "  tensorflow==2.15 tensorflow-probability==0.23.0 tflite-runtime keras==2.15 --python 3.11\n",
        "\n",
        "  from IPython.display import clear_output\n",
        "  clear_output()\n",
        "  import time\n",
        "  time.sleep(10)\n",
        "\n",
        "  import shutil\n",
        "  import os\n",
        "  shutil.rmtree(os.path.expanduser(\"~/.cache/pypoetry\"), ignore_errors=True)\n",
        "  shutil.rmtree(os.path.expanduser(\"~/.cache/pip\"), ignore_errors=True)\n",
        "  shutil.rmtree(os.path.expanduser(\"~/.cache/torch\"), ignore_errors=True)\n",
        "\n",
        "  with open(MARKER_FILE, \"w\") as f:\n",
        "    f.write(\"done\")\n",
        "\n",
        "  import IPython\n",
        "  IPython.Application.instance().kernel.do_shutdown(True)"
      ],
      "metadata": {
        "id": "Xs-SanaDRA3M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title General purpose (train)\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "def train_step(model: torch.nn.Module,\n",
        "               data_loader: torch.utils.data.DataLoader,\n",
        "               loss_fn: torch.nn.Module,\n",
        "               optimizer: torch.optim.Optimizer,\n",
        "               accuracy_fn,\n",
        "               device: torch.device):\n",
        "  \"\"\"Performs a training with model trying to learn on data_loader\"\"\"\n",
        "  train_loss, train_acc = 0, 0\n",
        "  model.train()\n",
        "  for batch, (X, y) in enumerate(data_loader):\n",
        "    X, y = X.to(device), y.to(device)\n",
        "    y_pred = model(X)\n",
        "    loss = loss_fn(y_pred, y)\n",
        "    acc = accuracy_fn(y, y_pred.argmax(dim=1))\n",
        "    train_loss += loss\n",
        "    train_acc += acc\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "  train_loss /= len(data_loader)\n",
        "  train_acc /= len(data_loader)\n",
        "  #print(f\"Train loss: {train_loss:.5f} | Train acc: {train_acc:.2f}\")\n",
        "  return train_loss, train_acc\n",
        "\n",
        "def test_step(model: torch.nn.Module,\n",
        "               data_loader: torch.utils.data.DataLoader,\n",
        "               loss_fn: torch.nn.Module,\n",
        "               accuracy_fn,\n",
        "               device: torch.device):\n",
        "  \"\"\"Performs a testing with model\"\"\"\n",
        "  test_loss, test_acc = 0, 0\n",
        "  model.eval()\n",
        "  with torch.inference_mode():\n",
        "    for (X, y) in data_loader:\n",
        "      X, y = X.to(device), y.to(device)\n",
        "      test_pred = model(X)\n",
        "      test_loss += loss_fn(test_pred, y)\n",
        "      test_acc += accuracy_fn(y, test_pred.argmax(dim=1))\n",
        "\n",
        "    test_loss /= len(data_loader)\n",
        "    test_acc /= len(data_loader)\n",
        "    #print(f\"Test loss: {test_loss:.5f} | Test acc: {test_acc:.2f}\\n\")\n",
        "  return test_loss, test_acc\n",
        "\n",
        "def model_estim(model, epochs, train_dataloader, test_dataloader, loss_fn, optimizer, accuracy_fn, device):\n",
        "  train_losses, test_losses = [], []\n",
        "  train_accs, test_accs = [], []\n",
        "\n",
        "  for epoch in tqdm(range(epochs)):\n",
        "    train_loss, train_acc = train_step(model=model,\n",
        "              data_loader=train_dataloader,\n",
        "              loss_fn=loss_fn,\n",
        "              optimizer=optimizer,\n",
        "              accuracy_fn=accuracy_fn,\n",
        "              device=device)\n",
        "    test_loss, test_acc = test_step(model=model,\n",
        "              data_loader=test_dataloader,\n",
        "              loss_fn=loss_fn,\n",
        "              accuracy_fn=accuracy_fn,\n",
        "              device=device)\n",
        "    train_losses.append(train_loss.detach().cpu())\n",
        "    test_losses.append(test_loss.detach().cpu())\n",
        "    train_accs.append(train_acc.detach().cpu())\n",
        "    test_accs.append(test_acc.detach().cpu())\n",
        "  return train_losses, test_losses, train_accs, test_accs\n",
        "\n",
        "from timeit import default_timer as timer\n",
        "def train_model(model, epochs, train_dataloader, test_dataloader, loss_fn, optimizer, accuracy_fn, device):\n",
        "  start_time = timer()\n",
        "  train_losses, test_losses, train_accs, test_accs = model_estim(model=model,\n",
        "                                                                 epochs=epochs,\n",
        "                                                                 train_dataloader=train_dataloader,\n",
        "                                                                 test_dataloader=test_dataloader,\n",
        "                                                                 loss_fn=loss_fn,\n",
        "                                                                 optimizer=optimizer,\n",
        "                                                                 accuracy_fn=accuracy_fn,\n",
        "                                                                 device=device)\n",
        "\n",
        "  end_time = timer()\n",
        "  total_training_time_model_1 = print(f\"\\nTrain time on {device}: {(end_time - start_time):.3f} seconds\")\n",
        "\n",
        "  epochs_ = np.array(range(epochs))\n",
        "  fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "  axes[0].plot(epochs_, np.array(train_losses), label=\"train\", color=\"red\")\n",
        "  axes[0].plot(epochs_, np.array(test_losses), label=\"test\", color=\"blue\")\n",
        "  axes[0].set_title('Train and test loss')\n",
        "  axes[0].set_xlabel('Epoch')\n",
        "  axes[0].set_ylabel('Loss')\n",
        "  axes[0].set_xticks(epochs_)\n",
        "  axes[0].legend()\n",
        "\n",
        "  axes[1].plot(epochs_, np.array(train_accs), label=\"train\", color=\"red\")\n",
        "  axes[1].plot(epochs_, np.array(test_accs), label=\"test\", color=\"blue\")\n",
        "  axes[1].set_title('Train and test accuracy')\n",
        "  axes[1].set_xlabel('Epoch')\n",
        "  axes[1].set_ylabel('Loss')\n",
        "  axes[1].set_xticks(epochs_)\n",
        "  axes[1].legend()\n",
        "  plt.show()\n",
        "  torch.save(obj=model_1.state_dict(), f='EMNIST_recognition.pth')\n"
      ],
      "metadata": {
        "id": "MGnCyyyPJtXe",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title General purpose (visualisation, save, load)\n",
        "import numpy as np\n",
        "import torch\n",
        "from torchmetrics import ConfusionMatrix\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn as nn\n",
        "import onnx\n",
        "from onnx_tf.backend import prepare\n",
        "import tensorflow as tf\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "\n",
        "\n",
        "def get_conf_mat_torch(model_):\n",
        "  y_preds = []\n",
        "  model_.eval()\n",
        "  with torch.inference_mode():\n",
        "    for X,y in test_dataloader:\n",
        "      X, y = X.to(device), y.to(device)\n",
        "      y_logit = model_(X)\n",
        "      y_pred = y_logit.argmax(dim=1)\n",
        "      y_preds.append(y_pred.cpu())\n",
        "\n",
        "  y_pred_tensor = torch.cat(y_preds)\n",
        "  return y_pred_tensor\n",
        "\n",
        "def get_conf_mat_onnx(ort_session):\n",
        "  y_preds = []\n",
        "  for X, y in test_dataloader:\n",
        "      X_np = X.numpy() if isinstance(X, torch.Tensor) else X\n",
        "      input_name = ort_session.get_inputs()[0].name\n",
        "      outputs = ort_session.run(None, {input_name: X_np})\n",
        "      y_pred = outputs[0].argmax(axis=1)\n",
        "      y_preds.append(torch.tensor(y_pred))\n",
        "\n",
        "  y_pred_tensor = torch.cat(y_preds)\n",
        "  return y_pred_tensor\n",
        "\n",
        "def get_conf_mat_tflite(interpreter):\n",
        "  y_preds = []\n",
        "  input_details = interpreter.get_input_details()\n",
        "  output_details = interpreter.get_output_details()\n",
        "  scale, zero_point = input_details[0]['quantization']\n",
        "  dtype = input_details[0]['dtype']\n",
        "\n",
        "  for X, y in test_dataloader:\n",
        "      X_np = X.numpy()\n",
        "      X_q = X_np / scale + zero_point\n",
        "      X_q = np.clip(X_q, np.iinfo(dtype).min, np.iinfo(dtype).max)\n",
        "      X_q = X_q.astype(dtype)\n",
        "\n",
        "      interpreter.resize_tensor_input(input_details[0]['index'], X_q.shape)\n",
        "      interpreter.allocate_tensors()\n",
        "\n",
        "      interpreter.set_tensor(input_details[0]['index'], X_q)\n",
        "      interpreter.invoke()\n",
        "      output_data = interpreter.get_tensor(output_details[0]['index'])\n",
        "\n",
        "      out_scale, out_zero = output_details[0]['quantization']\n",
        "      output_data = (output_data.astype(np.float32) - out_zero) * out_scale\n",
        "\n",
        "      y_pred = np.argmax(output_data, axis=1)\n",
        "      y_preds.append(torch.tensor(y_pred))\n",
        "\n",
        "  y_pred_tensor = torch.cat(y_preds)\n",
        "\n",
        "  return y_pred_tensor\n",
        "\n",
        "def print_metrics(class_names, y_pred_tensor):\n",
        "  from torchmetrics.classification import MulticlassAccuracy, MulticlassPrecision, MulticlassRecall, MulticlassF1Score\n",
        "  num_classes = len(class_names)\n",
        "  accuracy = MulticlassAccuracy(num_classes=num_classes)\n",
        "  precision = MulticlassPrecision(num_classes=num_classes, average='macro')\n",
        "  recall = MulticlassRecall(num_classes=num_classes, average='macro')\n",
        "  f1 = MulticlassF1Score(num_classes=num_classes, average='macro')\n",
        "\n",
        "  acc_value = accuracy(y_pred_tensor, test_data.targets)\n",
        "  prec_value = precision(y_pred_tensor, test_data.targets)\n",
        "  rec_value = recall(y_pred_tensor, test_data.targets)\n",
        "  f1_value = f1(y_pred_tensor, test_data.targets)\n",
        "\n",
        "  print(f\"Accuracy:  {acc_value:.4f}\")\n",
        "  print(f\"Precision: {prec_value:.4f}\")\n",
        "  print(f\"Recall:    {rec_value:.4f}\")\n",
        "  print(f\"F1-score:  {f1_value:.4f}\")\n",
        "\n",
        "def draw_conf_tensor(confmat_tensor):\n",
        "  plt.figure(figsize=(8, 6))\n",
        "  plt.imshow(confmat_tensor.numpy(), cmap='viridis', interpolation='nearest', )\n",
        "  plt.colorbar()  # Add a color bar\n",
        "  plt.title('Confusion matrix')\n",
        "  plt.xlabel('predicted')\n",
        "  plt.ylabel('true')\n",
        "  plt.xticks(ticks=range(len(class_names)), labels=class_names)\n",
        "  plt.yticks(ticks=range(len(class_names)), labels=class_names)\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "au5Q4xY0xEmS",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#LeNet Example"
      ],
      "metadata": {
        "id": "w7UvtnDvIwAl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Load Dataset\n",
        "\n",
        "import os\n",
        "import torch\n",
        "from torch import nn\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.transforms import ToTensor\n",
        "from torch.utils.data import DataLoader\n",
        "from timeit import default_timer as timer\n",
        "import torchmetrics, mlxtend\n",
        "from torchmetrics import ConfusionMatrix\n",
        "from mlxtend.plotting import plot_confusion_matrix\n",
        "from tqdm.auto import tqdm\n",
        "import random\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from torchmetrics import Accuracy\n",
        "from torchinfo import summary\n",
        "import numpy as np\n",
        "\n",
        "from google.colab import output\n",
        "output.clear()\n",
        "\n",
        "train_data = datasets.EMNIST(\n",
        "    root=\"data\",\n",
        "    split=\"bymerge\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=ToTensor(),\n",
        "    target_transform=None\n",
        ")\n",
        "\n",
        "test_data = datasets.EMNIST(\n",
        "    root=\"data\",\n",
        "    split=\"bymerge\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=ToTensor(),\n",
        "    target_transform=None\n",
        ")\n",
        "\n",
        "class_names = train_data.classes\n",
        "num_classes = len(class_names)\n",
        "print(f\"Train casses : {len(train_data)} \\nTest cases: {len(test_data)} \\nLabels: {class_names}\" )\n",
        "\n",
        "torch.manual_seed(42)\n",
        "fig = plt.figure(figsize=(12,12))\n",
        "rows, cols = 4, 4\n",
        "for i in range(1, rows*cols+1):\n",
        "  random_idx = torch.randint(0, len(train_data), size=[1]).item()\n",
        "  img, label = train_data[random_idx]\n",
        "  fig.add_subplot(rows, cols, i)\n",
        "  plt.imshow(img.squeeze().T)\n",
        "  plt.title(class_names[label])\n",
        "  plt.axis(False)\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "train_dataloader = DataLoader(dataset=train_data,\n",
        "                              batch_size=BATCH_SIZE,\n",
        "                              shuffle=True,\n",
        "                              num_workers=os.cpu_count())\n",
        "\n",
        "test_dataloader = DataLoader(dataset=test_data,\n",
        "                             batch_size=BATCH_SIZE,\n",
        "                             shuffle=False,\n",
        "                             num_workers=os.cpu_count())\n",
        "\n",
        "print(f\"DataLoaders: {train_dataloader, test_dataloader}\")\n",
        "print(f\"Length of train_dataloader {len(train_dataloader)} batches of {BATCH_SIZE}...\")\n",
        "print(f\"Length of test_dataloader {len(test_dataloader)} batches of {BATCH_SIZE}...\")\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "metadata": {
        "id": "48TeMldcP8Ob",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Convolution\n",
        "$$\n",
        "\\mathbf{Y} = \\text{Conv}_{\\mathbf{w}}(\\mathbf{X}); \\quad\n",
        "Y_{i,j} = \\sum_{\\alpha=-1}^1 \\sum_{\\beta=-1}^1 X_{i+\\alpha, j+\\beta} w_{\\alpha,\\beta}\n",
        "$$\n",
        "* ReLU\n",
        "$$\n",
        "\\mathbf{Y} = \\text{ReLU}(\\mathbf{X}); \\quad\n",
        "Y_{i,j} = X_{i,j} \\Theta(X_{i,j}),\n",
        "$$\n",
        "where $\\Theta$ is the Heaviside function\n",
        "* MaxPool\n",
        "$$\n",
        "\\mathbf{Y} = \\text{MaxPool}(\\mathbf{X}); \\quad\n",
        "Y_{i,j} = \\max\\{X_{2i,2j};X_{2i+1,2j};X_{2i,2j+1};X_{2i+1,2j+1}\\}\n",
        "$$\n",
        "please note that in this case matrix $\\mathbf{Y}$ has different size.\n",
        "* Fully connected\n",
        "$$\n",
        "\\vec{Y} = \\text{Linear}_{\\mathbf{w}}(\\mathbf{X}); \\quad\n",
        "Y_i = \\sum_{i,j} X_{i,j} w_{i,j}\n",
        "$$\n",
        "<br><br><br>\n",
        "The whole neural network can be written down as\n",
        "$$\n",
        "\\vec{Y} = \\underbrace{\\text{Linear}_{\\mathbf{w''}}(\\text{MaxPool}(\\text{ReLU}(\\text{Conv}_{\\mathbf{w'}}(\\text{ReLU}(\\text{Conv}_{\\mathbf{w}}(\\mathbf{X}))))))}_{\\text{Neural Network}}\n",
        "$$"
      ],
      "metadata": {
        "id": "Z0yYO2XhOJEi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "class ConvolutionEMNISTModel(nn.Module):\n",
        "  def __init__(self, input_shape: int, hidden_units: int, output_shape: int):\n",
        "    super().__init__()\n",
        "    self.layer_stack = nn.Sequential(\n",
        "      nn.Conv2d(in_channels=input_shape,\n",
        "                out_channels=hidden_units,\n",
        "                kernel_size=3,\n",
        "                stride=1,\n",
        "                padding=1),\n",
        "      nn.ReLU(),\n",
        "      nn.Conv2d(in_channels=hidden_units,\n",
        "                out_channels=hidden_units,\n",
        "                kernel_size=3,\n",
        "                stride=1,\n",
        "                padding=1),\n",
        "      nn.ReLU(),\n",
        "      nn.MaxPool2d(kernel_size=2),\n",
        "      nn.Flatten(),\n",
        "      nn.LazyLinear(out_features=output_shape)\n",
        "      )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.layer_stack(x)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "metadata": {
        "id": "Rs0YM58osfb7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Model\n",
        "torch.manual_seed(42)\n",
        "model_1 = ConvolutionEMNISTModel(input_shape=1,\n",
        "                              hidden_units=10,\n",
        "                              output_shape=len(class_names)).to(device)"
      ],
      "metadata": {
        "id": "1_sj7OxHyZ6s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title NN Structure\n",
        "\n",
        "from torchview import draw_graph\n",
        "model_graph = draw_graph(model_1, input_size=[1,1,28,28], expand_nested=True, graph_dir='LR',)\n",
        "model_graph.resize_graph(scale=5.0)\n",
        "model_graph.visual_graph"
      ],
      "metadata": {
        "id": "qTg_byfEumeQ",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Categorical cross-entropy** loss function $L$ is computed as follows\n",
        "$$\n",
        "L = - \\sum_i t_i \\ln(\\sigma_i),\n",
        "$$\n",
        "where $t_i$ are truth values and $\\sigma_i$ are **softmax** outputs\n",
        "$$\n",
        "\\sigma_i(\\mathbf{z}) = \\frac{e^{z_i}}{\\sum_{j=1}^K e^{z_j}} \\text{ for } i = 1, \\dotsc, K \\text{ and } \\mathbf{z} = (z_1, \\dotsc, z_K) \\in \\mathbb{R}^K.\n",
        "$$\n",
        "In our case $t_i$ are either $0$ or $1$ (see `to_categorical` function). We may have used `sparse_crossentropy`, same function different class encoding."
      ],
      "metadata": {
        "id": "qIG1FeKuqj_R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Train (very slow, better load pretrained)\n",
        "mode = \"Train\"  # @param ['Train', 'Preload']\n",
        "\n",
        "if mode == \"Train\":\n",
        "  loss_fn = nn.CrossEntropyLoss()\n",
        "  optimizer = torch.optim.SGD(params=model_1.parameters(), lr=0.1)\n",
        "  accuracy_fn = Accuracy(task=\"multiclass\", num_classes=num_classes).to(device)\n",
        "\n",
        "  train_model(model=model_1,\n",
        "              epochs=3,\n",
        "              train_dataloader=train_dataloader,\n",
        "              test_dataloader=test_dataloader,\n",
        "              loss_fn=loss_fn,\n",
        "              optimizer=optimizer,\n",
        "              accuracy_fn=accuracy_fn,\n",
        "              device=device)\n",
        "else:\n",
        "  !wget https://raw.githubusercontent.com/fbeilstein/presentations/refs/heads/master/presentation_for_December_3_2024/EMNIST_recognition.pth\n",
        "  model_1.load_state_dict(torch.load('EMNIST_recognition.pth', map_location=torch.device(device)))"
      ],
      "metadata": {
        "id": "GlwtBOD3v_Hb",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Quality\n",
        "\n",
        "y_pred_tensor = get_conf_mat_torch(model_1)\n",
        "confmat = ConfusionMatrix(task='multiclass', num_classes=len(class_names))\n",
        "confmat_tensor = confmat(preds=y_pred_tensor,\n",
        "                        target=test_data.targets)\n",
        "draw_conf_tensor(confmat_tensor)\n",
        "print_metrics(class_names, y_pred_tensor)"
      ],
      "metadata": {
        "id": "PXahErUfgjWo",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Internal Structure\n",
        "max_idx = len(test_data)\n",
        "cur_idx = 0\n",
        "\n",
        "\n",
        "from IPython.display import display\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import clear_output\n",
        "\n",
        "def plot_out(idx):\n",
        "  image, img_class = test_data[idx]\n",
        "  model_1.eval()\n",
        "  #image = np.transpose(image, (0,2,1))\n",
        "  x = image.unsqueeze(0).to(device)\n",
        "  fig = plt.figure(figsize=(10,6), constrained_layout=True)\n",
        "  grid = plt.GridSpec(10, 7, wspace=0, hspace=0, figure=fig,\n",
        "                      width_ratios=[1,1,1,1,1,1,10],\n",
        "                      height_ratios=[1,1,1,1,1,1,1,1,1,1])\n",
        "  ax_input = plt.subplot(grid[:,0], xticks=[], yticks=[])\n",
        "  ax_input.imshow(image[0].T)\n",
        "  ax_input.axis(False)\n",
        "  with torch.inference_mode():\n",
        "    for i, layer in enumerate(model_1.layer_stack):\n",
        "      x = layer(x)\n",
        "      for j, xx in enumerate(x[0]):\n",
        "        numpy_img = xx.cpu().detach().numpy()\n",
        "        if numpy_img.ndim != 2: continue\n",
        "        plt.subplot(grid[j,i+1], xticks=[], yticks=[])\n",
        "        plt.imshow(numpy_img.T, cmap=\"grey\")\n",
        "        plt.axis(False)\n",
        "  bin_edges = [x - 0.5 for x in range(48)]\n",
        "  bar_heights = torch.softmax(x.squeeze(), dim=0).detach().cpu().numpy()\n",
        "  bar_widths = [1] * 47\n",
        "  ax_result = plt.subplot(grid[:,6])#, xticks=class_names)\n",
        "  ax_result.bar(bin_edges[:-1], bar_heights, width=bar_widths, align='edge', edgecolor='black')\n",
        "  ax_result.set_xticks(ticks=list(range(len(class_names))), labels=class_names)\n",
        "  #ax_result.xlabel('Numbers')\n",
        "  #ax_result.ylabel('Probabilities')\n",
        "  plt.show()\n",
        "\n",
        "def clear_show_widgets():\n",
        "  clear_output()\n",
        "  buttonN = widgets.Button(description=\"Next\")\n",
        "  buttonP = widgets.Button(description=\"Previous\")\n",
        "  display(widgets.HBox([buttonN, buttonP]))\n",
        "  buttonN.on_click(on_button_clicked_next)\n",
        "  buttonP.on_click(on_button_clicked_previous)\n",
        "\n",
        "def on_button_clicked_next(*args):\n",
        "  global cur_idx\n",
        "  cur_idx = (1 + cur_idx) % max_idx\n",
        "  clear_show_widgets()\n",
        "  plot_out(cur_idx)\n",
        "\n",
        "def on_button_clicked_previous(*args):\n",
        "  global cur_idx\n",
        "  cur_idx = (cur_idx - 1 + max_idx) % max_idx\n",
        "  clear_show_widgets()\n",
        "  plot_out(cur_idx)\n",
        "\n",
        "\n",
        "clear_show_widgets()\n"
      ],
      "metadata": {
        "id": "yML7qYb7cWpS",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8s2cy7MY-Bla",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title #Visualization code\n",
        "import IPython\n",
        "from google.colab import output\n",
        "import numpy as np\n",
        "import json\n",
        "\n",
        "def process_symbol(num_img):\n",
        "  #num_img = center_image(num_img)\n",
        "  #img = torch.tensor(num_img, dtype=torch.float32).unsqueeze(0).unsqueeze(0).to(device)\n",
        "  x = torch.tensor(num_img.T, dtype=torch.float32).unsqueeze(0).unsqueeze(0).to(device)\n",
        "  model_1.eval()\n",
        "  #plt.figure(figsize=(15,10))\n",
        "  with torch.inference_mode():\n",
        "    for i, layer in enumerate(model_1.layer_stack):\n",
        "      x = layer(x)\n",
        "      for j, xx in enumerate(x[0]):\n",
        "        numpy_img = xx.cpu().detach().numpy()\n",
        "        if numpy_img.ndim != 2: continue\n",
        "        #plt.subplot(10, 10, 10*j+i+1)\n",
        "        #plt.imshow(numpy_img)\n",
        "        #plt.title(type(layer).__name__, fontsize=8)\n",
        "        #plt.axis(False)\n",
        "  #plt.show()\n",
        "  return x.argmax(dim=1).cpu().item()\n",
        "\n",
        "\n",
        "def do_recognition(array_8x8):\n",
        "  arr = np.transpose(np.array(array_8x8))\n",
        "  return IPython.display.JSON({'msg': process_symbol(arr)})\n",
        "\n",
        "main_str = '''\n",
        "<body>\n",
        "  \t<div id=\"paint\">\n",
        "  \t\t<canvas id=\"myCanvas\" width=\"600\" height=\"600\"\n",
        "              onmousedown=\"start_paint(event)\"\n",
        "              onmousemove=\"do_paint(event)\"\n",
        "              onmouseup=\"stop_paint(event)\"\n",
        "              style=\"border:3px solid #000000;\">\n",
        "      </canvas>\n",
        "\t  </div>\n",
        "    <div style=\"position:absolute; top:0px; left:625px;\">\n",
        "        <h2> Cropped image </h2>\n",
        "        <img id=\"partial_image\" style=\"width:80px; height:80px; border:3px solid #000000;\">\n",
        "        <h2> Scaled </h2>\n",
        "        <canvas id=\"bin_canvas\" width=\"80\" height=\"80\"\n",
        "              style=\"border:3px solid #000000;\">\n",
        "        </canvas>\n",
        "        <h2> Recognize symbol </h2>\n",
        "        <button type=\"button\" onclick=\"process_data();\">\n",
        "        Do recognition\n",
        "        </button>\n",
        "        <h2> Clean canvas </h2>\n",
        "        <button type=\"button\" onclick=\"clear_canvas();\">\n",
        "        Clean\n",
        "        </button>\n",
        "        <h2> Recognition result </h2>\n",
        "        <div id=\"rec_res\" style=\"font-size:40px;\"></div>\n",
        "    </div>\n",
        "<script>\n",
        "\n",
        "var canvas = document.getElementById('myCanvas');\n",
        "var ctx = canvas.getContext('2d');\n",
        "var canvas_2 = document.getElementById(\"bin_canvas\");\n",
        "var ctx_2 = canvas_2.getContext(\"2d\");\n",
        "var mouse = {x: 0, y: 0, state: 0, min_x: 600, max_x: 0, min_y: 600, max_y: 0};\n",
        "var full_image = ctx.getImageData(0, 0, 600, 600);\n",
        "var partial_image = document.getElementById('partial_image');\n",
        "var binarized = null;\n",
        "\n",
        "var N_scaled = 28;\n",
        "\n",
        "ctx.lineJoin = 'round';\n",
        "ctx.lineCap = 'round';\n",
        "\n",
        "\n",
        "function start_paint(e)\n",
        "{\n",
        "  ctx.putImageData(full_image, 0, 0);\n",
        "  mouse.state = 1;\n",
        "  ctx.beginPath();\n",
        "  ctx.moveTo(mouse.x, mouse.y);\n",
        "}\n",
        "\n",
        "var line_width = 35;//28;\n",
        "\n",
        "function do_paint(e)\n",
        "{\n",
        "  mouse.x = e.pageX - 10;\n",
        "  mouse.y = e.pageY - 10;\n",
        "  if (!mouse.state)\n",
        "    return;\n",
        "  ctx.lineTo(mouse.x, mouse.y);\n",
        "  ctx.lineWidth = line_width;\n",
        "  ctx.strokeStyle = '#000000';\n",
        "  ctx.stroke();\n",
        "  full_image = ctx.getImageData(0, 0, 600, 600);\n",
        "  if (mouse.min_x > mouse.x)\n",
        "    mouse.min_x = mouse.x;\n",
        "  if (mouse.min_y > mouse.y)\n",
        "    mouse.min_y = mouse.y;\n",
        "  if (mouse.max_x < mouse.x)\n",
        "    mouse.max_x = mouse.x;\n",
        "  if (mouse.max_y < mouse.y)\n",
        "    mouse.max_y = mouse.y;\n",
        "}\n",
        "\n",
        "function stop_paint(e)\n",
        "{\n",
        "  mouse.state = 0;\n",
        "  full_image = ctx.getImageData(0, 0, 600, 600);\n",
        "\n",
        "  c_x = (mouse.max_x + mouse.min_x) / 2;\n",
        "  c_y = (mouse.max_y + mouse.min_y) / 2;\n",
        "  a = Math.max(mouse.max_x - mouse.min_x, mouse.max_y - mouse.min_y);\n",
        "  a += line_width;// + 90;\n",
        "  var part = ctx.getImageData(c_x - a/2, c_y - a/2, a, a);\n",
        "  partial_image.src = getImageURL(part);\n",
        "\n",
        "  binarized = Array(N_scaled).fill(0).map(x => Array(N_scaled).fill(0))\n",
        "  var counts = Array(N_scaled).fill(0).map(x => Array(N_scaled).fill(0))\n",
        "\n",
        "  for (var idx_y = 0; idx_y < part.height; idx_y += 1)\n",
        "    for (var idx_x = 0; idx_x < part.width; idx_x += 1)\n",
        "    {\n",
        "      counts[Math.floor(N_scaled * idx_x / part.width)][Math.floor(N_scaled * idx_y / part.height)] += 1;\n",
        "      if (part.data[idx_y * part.width * 4 + idx_x * 4 + 3] > 1)\n",
        "      \tbinarized[Math.floor(N_scaled * idx_x / part.width)][Math.floor(N_scaled * idx_y / part.height)] += 1;\n",
        "    }\n",
        "\n",
        "  for (var i = 0; i < N_scaled; i++)\n",
        "    for (var j = 0; j < N_scaled; j++)\n",
        "      binarized[i][j] = binarized[i][j] / counts[i][j];\n",
        "\n",
        "  ctx_2.fillStyle = \"rgba(255, 255, 255, 255)\";\n",
        "  ctx_2.fillRect(0, 0, canvas_2.width, canvas_2.height);\n",
        "  for (var i = 0; i < N_scaled; i++)\n",
        "    for (var j = 0; j < N_scaled; j++)\n",
        "    {\n",
        "      var c = Math.floor(255 - 255*binarized[i][j]);\n",
        "      var x = Math.floor(canvas_2.width / N_scaled * i);\n",
        "      var y = Math.floor(canvas_2.height / N_scaled * j);\n",
        "      var dx = Math.floor(canvas_2.width / N_scaled + 1);\n",
        "      var dy = Math.floor(canvas_2.height / N_scaled + 1);\n",
        "      ctx_2.fillStyle = \"rgba(\" + [c, c, c, 255].join(\",\") + \")\";\n",
        "      ctx_2.fillRect(x, y, dx, dy);\n",
        "    }\n",
        "\n",
        "  ctx.lineWidth = 2;\n",
        "  ctx.strokeStyle = '#FF0000';\n",
        "  ctx.strokeRect(c_x - a/2, c_y - a/2, a, a);\n",
        "}\n",
        "\n",
        "function getImageURL(imgData)\n",
        "{\n",
        "   var canvas = document.createElement('canvas');\n",
        "   var ctx = canvas.getContext('2d');\n",
        "   canvas.width = imgData.width;\n",
        "   canvas.height = imgData.height;\n",
        "   ctx.putImageData(imgData, 0, 0);\n",
        "   return canvas.toDataURL(); //image URL\n",
        "}\n",
        "\n",
        "async function process_data()\n",
        "{\n",
        "  const result = await google.colab.kernel.invokeFunction('notebook.DoRec', [binarized], {});\n",
        "  params = result.data['application/json'];\n",
        "  document.getElementById('rec_res').innerHTML = \"0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZabdefghnqrt\"[params.msg];\n",
        "}\n",
        "\n",
        "function clear_canvas()\n",
        "{\n",
        "  mouse = {x: 0, y: 0, state: 0, min_x: 600, max_x: 0, min_y: 600, max_y: 0};\n",
        "  binarized = null;\n",
        "  ctx.clearRect(0, 0, canvas.width, canvas.height);\n",
        "  full_image = ctx.getImageData(0, 0, 600, 600);\n",
        "  ctx_2.clearRect(0, 0, 80, 80);\n",
        "  var part = ctx.getImageData(0, 0, 80, 80);\n",
        "  partial_image.src = getImageURL(part);\n",
        "  document.getElementById('rec_res').innerHTML = \"\";\n",
        "}\n",
        "\n",
        "    </script>\n",
        "  </body>\n",
        "'''\n",
        "\n",
        "display(IPython.display.HTML(main_str))\n",
        "output.register_callback('notebook.DoRec', do_recognition)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Structured pruning w/o deletion\n",
        "\n",
        "import torch\n",
        "import torch.nn.utils.prune as prune\n",
        "import torch.nn as nn\n",
        "import copy\n",
        "\n",
        "model_pruned_lazy = copy.deepcopy(model_1)\n",
        "\n",
        "def prune_all_conv_layers(model, amount=0.2):\n",
        "    for name, module in model.named_modules():\n",
        "        if isinstance(module, nn.Conv2d):\n",
        "            prune.ln_structured(module, name='weight', amount=amount, n=2, dim=0)\n",
        "            prune.remove(module, 'weight')\n",
        "            print(f\"Pruned {amount*100}% of filters in layer: {name}\")\n",
        "\n",
        "\n",
        "prune_all_conv_layers(model_pruned_lazy, amount=0.5)"
      ],
      "metadata": {
        "id": "NmLcx56Z0lKQ",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Structured pruning w deletion\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "def structured_prune_model(model, amount: float):\n",
        "    old_layers = [l for l in model.layer_stack if isinstance(l, nn.Conv2d)]\n",
        "    device = next(model.parameters()).device\n",
        "\n",
        "    in_ch = old_layers[0].in_channels\n",
        "    out_ch = old_layers[-1].out_channels\n",
        "    hidden_units = old_layers[0].out_channels\n",
        "\n",
        "    # --- 1. Calculate importance of filters in the first Conv2d\n",
        "    with torch.no_grad():\n",
        "        w = old_layers[0].weight.abs().sum(dim=(1, 2, 3)).cpu().numpy()\n",
        "        n_prune = int(len(w) * amount)\n",
        "        keep_idx = np.argsort(-w)[:-n_prune] if n_prune > 0 else np.arange(len(w))\n",
        "\n",
        "    new_hidden = len(keep_idx)\n",
        "\n",
        "    # --- 2. Create new model\n",
        "    new_model = ConvolutionEMNISTModel(\n",
        "        input_shape=in_ch,\n",
        "        hidden_units=new_hidden,\n",
        "        output_shape=model.layer_stack[-1].out_features\n",
        "    ).to(device)\n",
        "\n",
        "    # --- 3. Copy filters\n",
        "    with torch.no_grad():\n",
        "        new_model.layer_stack[0].weight.copy_(old_layers[0].weight[keep_idx])\n",
        "        new_model.layer_stack[0].bias.copy_(old_layers[0].bias[keep_idx])\n",
        "\n",
        "        new_model.layer_stack[2].weight.copy_(old_layers[1].weight[keep_idx][:, keep_idx])\n",
        "        new_model.layer_stack[2].bias.copy_(old_layers[1].bias[keep_idx])\n",
        "\n",
        "    print(f\"\\nPruned {n_prune}/{len(w)} filters → new hidden_units = {new_hidden}\")\n",
        "    return new_model\n",
        "\n",
        "\n",
        "print(\"Before pruning:\")\n",
        "for l in model_1.layer_stack:\n",
        "    if isinstance(l, nn.Conv2d):\n",
        "        print(f\"{l.in_channels} → {l.out_channels}\")\n",
        "\n",
        "model_pruned = structured_prune_model(model_1, amount=0.3)\n",
        "\n",
        "print(\"\\nAfter pruning:\")\n",
        "for l in model_pruned.layer_stack:\n",
        "    if isinstance(l, nn.Conv2d):\n",
        "        print(f\"{l.in_channels} → {l.out_channels}\")\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "zI7ikuvWFlvN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Summary:\n",
        "from torchinfo import summary\n",
        "print(\"Before:\")\n",
        "print(summary(model_1, input_size=(1, 1, 28, 28), col_names=(\"input_size\", \"output_size\", \"num_params\")))\n",
        "print(\"\\n\\nAfter:\")\n",
        "print(summary(model_pruned, input_size=(1, 1, 28, 28), col_names=(\"input_size\", \"output_size\", \"num_params\")))"
      ],
      "metadata": {
        "cellView": "form",
        "id": "cYfNYy1MUElh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Quality\n",
        "\n",
        "y_pred_tensor = get_conf_mat_torch(model_pruned)\n",
        "confmat = ConfusionMatrix(task='multiclass', num_classes=len(class_names))\n",
        "confmat_tensor = confmat(preds=y_pred_tensor,\n",
        "                        target=test_data.targets)\n",
        "draw_conf_tensor(confmat_tensor)\n",
        "print_metrics(class_names, y_pred_tensor)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "TgK81z811cvo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Train pruned model (slow)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(params=model_pruned_lazy.parameters(), lr=0.1)\n",
        "accuracy_fn = Accuracy(task=\"multiclass\", num_classes=num_classes).to(device)\n",
        "\n",
        "\n",
        "train_model(model=model_pruned_lazy,\n",
        "            epochs=1,\n",
        "            train_dataloader=train_dataloader,\n",
        "            test_dataloader=test_dataloader,\n",
        "            loss_fn=loss_fn,\n",
        "            optimizer=optimizer,\n",
        "            accuracy_fn=accuracy_fn,\n",
        "            device=device)"
      ],
      "metadata": {
        "id": "_knAP-k3WF0s",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model saving / loading"
      ],
      "metadata": {
        "id": "Z82CH408JI0J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Save model to onnx\n",
        "import warnings\n",
        "\n",
        "def save_to_onnx(model, filename, device):\n",
        "  dummy_input = torch.randn(1, 1, 28, 28) # see test_data\n",
        "  model.to('cpu').eval()\n",
        "  with warnings.catch_warnings():\n",
        "    warnings.simplefilter(\"ignore\", DeprecationWarning)\n",
        "    torch.onnx.export(\n",
        "      model,\n",
        "      dummy_input,\n",
        "      filename,\n",
        "      input_names=[\"input\"],\n",
        "      output_names=[\"output\"],\n",
        "      dynamic_axes={\n",
        "          \"input\": {0: \"batch_size\"},   # dynamic batch size\n",
        "          \"output\": {0: \"batch_size\"}\n",
        "      },\n",
        "      do_constant_folding=True,\n",
        "      dynamo=False, # old exporter, new crashes and buggy\n",
        "      opset_version=11  # some layers may need >= 11\n",
        "    )\n",
        "  model.to(device)\n",
        "\n",
        "save_to_onnx(model_1, \"my_lenet.onnx\", device)"
      ],
      "metadata": {
        "id": "NJy9YROp9ryT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Load onnx model\n",
        "import onnx\n",
        "import onnxruntime as ort\n",
        "\n",
        "ort_session = ort.InferenceSession(\"my_lenet.onnx\")"
      ],
      "metadata": {
        "id": "1AWfbEA5-bLs",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Quality\n",
        "\n",
        "y_pred_tensor = get_conf_mat_onnx(ort_session)\n",
        "confmat = ConfusionMatrix( task='multiclass', num_classes=len(class_names))\n",
        "confmat_tensor = confmat(preds=y_pred_tensor,\n",
        "                        target=test_data.targets)\n",
        "draw_conf_tensor(confmat_tensor)\n",
        "print_metrics(class_names, y_pred_tensor)"
      ],
      "metadata": {
        "id": "yXXUW5Ii5NYk",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Quantization"
      ],
      "metadata": {
        "id": "Sq2lGWZbI1Uh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Save quantize model to tflite\n",
        "\n",
        "\n",
        "def onnx_to_tflite(onnx_name, train_data):\n",
        "  onnx_model = onnx.load(onnx_name)\n",
        "  tf_rep = prepare(onnx_model)\n",
        "  tf_rep.export_graph(\"model_tf\")\n",
        "  print(\"✅ Converted to TensorFlow\")\n",
        "\n",
        "  def representative_data_gen():\n",
        "    for i, (x, _) in enumerate(train_data):\n",
        "      if i > 200:  # 200 samples is enough\n",
        "        break\n",
        "      yield [x.unsqueeze(0).numpy()]\n",
        "\n",
        "  # TensorFlow → TFLite\n",
        "  converter = tf.lite.TFLiteConverter.from_saved_model(\"model_tf\")\n",
        "  converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "  converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
        "  converter.representative_dataset = representative_data_gen\n",
        "  converter.inference_input_type = tf.int8\n",
        "  converter.inference_output_type = tf.int8\n",
        "\n",
        "  tflite_model = converter.convert()\n",
        "  with open(\"my_lenet.tflite\", \"wb\") as f:\n",
        "      f.write(tflite_model)\n",
        "  print(\"✅ Converted to model.tflite\")\n",
        "\n",
        "\n",
        "onnx_to_tflite(\"my_lenet.onnx\", train_data)"
      ],
      "metadata": {
        "id": "CaT7Gg89RSmS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Load quantized model from tflite\n",
        "import tensorflow as tf\n",
        "\n",
        "interpreter = tf.lite.Interpreter(model_path=\"my_lenet.tflite\")\n",
        "interpreter.allocate_tensors()"
      ],
      "metadata": {
        "id": "FaxPeDTXax0D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Quality\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torchmetrics import ConfusionMatrix\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "y_pred_tensor = get_conf_mat_tflite(interpreter)\n",
        "confmat = ConfusionMatrix( task='multiclass', num_classes=len(class_names))\n",
        "confmat_tensor = confmat(preds=y_pred_tensor,\n",
        "                        target=test_data.targets)\n",
        "\n",
        "draw_conf_tensor(confmat_tensor)\n",
        "print_metrics(class_names, y_pred_tensor)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Weoh2ly26O-z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Knowledge distillation"
      ],
      "metadata": {
        "id": "pVgpKFReIt5w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Student model\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class MLPStudent(nn.Module):\n",
        "    def __init__(self, num_classes=47):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(28*28, 128)\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        self.fc3 = nn.Linear(64, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.size(0), -1)  # flatten\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "student = MLPStudent().to(device)"
      ],
      "metadata": {
        "id": "ziuwJRjooK_j",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Student model structure\n",
        "\n",
        "from torchview import draw_graph\n",
        "model_graph = draw_graph(student, input_size=[1,1,28,28], expand_nested=True, graph_dir='LR',)\n",
        "model_graph.resize_graph(scale=5.0)\n",
        "model_graph.visual_graph\n",
        "\n"
      ],
      "metadata": {
        "id": "lCDfcXpWHz7h",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Knowledge distillation / training (long)\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "\n",
        "teacher = model_1\n",
        "optimizer = torch.optim.Adam(student.parameters(), lr=1e-3)\n",
        "\n",
        "def cross_entropy_loss(student_logits, labels):\n",
        "    return F.cross_entropy(student_logits, labels)\n",
        "\n",
        "def kl_divergence_loss(student_logits, teacher_logits, T=4.0):\n",
        "    soft_teacher = F.softmax(teacher_logits / T, dim=1)\n",
        "    soft_student = F.log_softmax(student_logits / T, dim=1)\n",
        "    return F.kl_div(soft_student, soft_teacher, reduction='batchmean') * (T * T)\n",
        "\n",
        "def distillation_loss(student_logits, teacher_logits, labels, alpha=0.7, T=4.0):\n",
        "    loss_ce = cross_entropy_loss(student_logits, labels)\n",
        "    loss_kl = kl_divergence_loss(student_logits, teacher_logits, T)\n",
        "    loss = alpha * loss_ce + (1 - alpha) * loss_kl\n",
        "    return loss, loss_ce, loss_kl\n",
        "\n",
        "def train_epoch_distill(student, teacher, dataloader, optimizer, device,\n",
        "                        alpha=0.7, T=4.0):\n",
        "    teacher.eval()\n",
        "    student.train()\n",
        "\n",
        "    total_loss, total_ce, total_kl = 0.0, 0.0, 0.0\n",
        "    total_correct, total_samples = 0, 0\n",
        "\n",
        "    for images, labels in tqdm(dataloader, desc=\"Training\"):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            teacher_logits = teacher(images)\n",
        "\n",
        "        student_logits = student(images)\n",
        "\n",
        "        loss, loss_ce, loss_kl = distillation_loss(\n",
        "            student_logits, teacher_logits, labels, alpha, T\n",
        "        )\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item() * images.size(0)\n",
        "        total_ce += loss_ce.item() * images.size(0)\n",
        "        total_kl += loss_kl.item() * images.size(0)\n",
        "\n",
        "        preds = student_logits.argmax(dim=1)\n",
        "        total_correct += (preds == labels).sum().item()\n",
        "        total_samples += labels.size(0)\n",
        "\n",
        "    avg_loss = total_loss / total_samples\n",
        "    avg_ce = total_ce / total_samples\n",
        "    avg_kl = total_kl / total_samples\n",
        "    acc = total_correct / total_samples\n",
        "\n",
        "    return {\n",
        "        \"loss\": avg_loss,\n",
        "        \"ce_loss\": avg_ce,\n",
        "        \"kl_loss\": avg_kl,\n",
        "        \"acc\": acc\n",
        "    }\n",
        "\n",
        "for epoch in range(2):\n",
        "    stats = train_epoch_distill(student, teacher, train_dataloader, optimizer, device,\n",
        "                                alpha=0.7, T=4.0)\n",
        "    print(f\"Epoch {epoch+1}: \"\n",
        "          f\"loss={stats['loss']:.4f}, acc={stats['acc']:.4f}, \"\n",
        "          f\"CE={stats['ce_loss']:.4f}, KL={stats['kl_loss']:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "iaHxhhGrnMTX",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Save student model to onnx\n",
        "save_to_onnx(student, \"student.onnx\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Ym1cMNiEp9eO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Load student model from onnx\n",
        "\n",
        "import onnx\n",
        "import onnxruntime as ort\n",
        "\n",
        "ort_session = ort.InferenceSession(\"student.onnx\")"
      ],
      "metadata": {
        "id": "-cWX-fslF_HC",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Quality\n",
        "y_pred_tensor = get_conf_mat_onnx(ort_session)\n",
        "confmat = ConfusionMatrix( task='multiclass', num_classes=len(class_names))\n",
        "confmat_tensor = confmat(preds=y_pred_tensor,\n",
        "                        target=test_data.targets)\n",
        "draw_conf_tensor(confmat_tensor)\n",
        "print_metrics(class_names, y_pred_tensor)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Iwq_VwdUGLDY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Train (very slow, better load pretrained)\n",
        "\n",
        "\n",
        "student1 = MLPStudent().to(device)\n",
        "train_model(model=student1,\n",
        "            epochs=3,\n",
        "            train_dataloader=train_dataloader,\n",
        "            test_dataloader=test_dataloader,\n",
        "            loss_fn=loss_fn,\n",
        "            optimizer=optimizer,\n",
        "            accuracy_fn=accuracy_fn,\n",
        "            device=device)\n"
      ],
      "metadata": {
        "id": "k5WhEbD-gszF",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}